---
layout: article                       # 不变
title:  "(PyTorch) CrossEntropyLoss Log-softmax and NLL"     # 文章标题
date:   2020-10-09 17:34:40 +0800     # 编写时间，不要超过当前系统时间，否则编译不通过
key:   softmax              # 文章的唯一 ID，不要重复了
aside:
  toc: true
category: [MLF, blog]                # 重点来了，这是类别标签（什么名字都可以，别和其他标签重了）
sidebar:
    nav: Blog
---

Softmax：  
$\sigma(z)_j = \frac{e^{z_j}}{\sum_1^k e^{z_k}}$<br>
Log-softmax:  
$\sigma(z)_j = z_j-\log{\sum_1^k e^{z_k}}$

In PyTorch:
```nn.CrossEntropyLoss``` is equivalent to the combination of ```F.log_softmax``` and ```F.nll_loss```

```F.nll_loss``` only computes $\sum_i -y_i$ (depends on reduction).
