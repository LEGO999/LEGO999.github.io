---
layout: article                       # 不变
title:  "Does adversarial defense improve robustness of DNNs?"     # 文章标题
date:   2021-09-29 17:34:40 +0800     # 编写时间，不要超过当前系统时间，否则编译不通过
key:   at_robustness               # 文章的唯一 ID，不要重复了
aside:
  toc: true
category: [ML, blog]                # 重点来了，这是类别标签（什么名字都可以，别和其他标签重了）
sidebar:
    nav: Blog
---
Deep Neural Networks (DNNs) are known to be prone to adversarial attacks. Adversarial attacks manipulate predictions of DNNs via adding crafted perturbations, as shown in Fig.1.
<p align = "center">
<img src="Adversarial_example0.PNG" width="70%">
</p>
<p align = "center">
<sup>Fig.1 An adversarial example generated by FGSM (source: <a href="https://arxiv.org/abs/1412.6572">Goodfellow et al., 2014</a>)</sup>
</p>
In essence, adversarial attacks optimize the following objective function to maximize the prediction error:

<p align = "center">

$$\mathbb{E}_{x, y}[\max_{\delta\in\Delta}\text{Loss} (f_{\theta}(x+\delta),y)]$$

</p>
Adversarial defenses could be solutions for adversarial attacks. It improves robustness of DNNs against the most confusing samples (adversarial samples). This article serves to investigate whether Adversarial defenses also improve the robustness of DNNs against other "mild" perturbations and uncertainty calibration. The following performance will be evaluated in our experiments:

* Adversarial robustness
* Uncertainty calibration for in-distribution input
* Out-of-distribution detection
* Robustness under distributional shift
* Rotation invariance

## Setup ##
### Defense Methods ###
The classical PGD adversarial training (<a href="https://arxiv.org/abs/1706.06083">Madry et al., 2017</a>) and the state-of-the-art DDPM data augmentation (<a href="https://arxiv.org/abs/2103.01946">Rebuffi et al., 2021</a>) are chosen in our experiments. The latter was first in the ranking of <a href="https://robustbench.github.io/">RobustBench</a> leaderboard when the investigation was conducted.

### Evaluation ###
#### Adversarial Robustness ####
Untargeted PGD attack (<a href="https://arxiv.org/abs/1706.06083">Madry et al., 2017</a>) with $\text{L}_{\inf}=8/255$  is used to confirm whether trained models are adversarial robust. The number of iterations is set to seven. The metric of adversarial robustness is robust accuracy ($\uparrow$).
#### Uncertainty Calibration for In-distribution Input ####
Modern DNNs are often miscalibrated (overconfident). To be specific, DNNs predict higher likelihood than actual accuracy. Expected Calibrated Error (ECE) ($\downarrow$) was proposed to measure the deviation of likelihood from accuracy. It could be calculated using the following equation:
<p align = "center">

$$\text{ECE} = \sum_{m=1}^{M}\frac{\left|n_m\right|}{B}\left|\text{acc}_m-\text{conf}_m\right|$$

</p>

where $M$ is the number of bins, $n_m$ is the number of samples falling into the bin $m$, $\text{acc}_m$ and $\text{conf}_m$ are the mean accuracy and mean confidence (top-1 predictive probability) of the bin $m$ respectively. We adopt the static binning scheme with 100 bins in our experiments. As stated in <a href="https://arxiv.org/abs/2006.13092">Patel et al., 2020</a>, ECE estimation will be insensitive to binning scheme with 100 bins.
#### Out-of-distribution Detection ####
DNNs are trained on a training set and requested to predict on both in-distribution and out-of-distribution test sets. A robust classifier should well distinguish between two test sets. In this article, Area Under Receiver Characteristic Curve (AUROC) ($\uparrow$) is taken as the metric for out-of-distribution detection. The entropy of predictions is selected as the threshold in the AUROC.
#### Robustness under Distributional Shift ####
CIFAR-C/ImageNet-C dataset (<a href="https://arxiv.org/abs/1903.12261">hendrycks et al., 2019</a>) incorporates 19 kinds of common corruptions with five different severities in the real-world environment. With evaluation with corrupted data, we could examine generalization (accuracy) ($\uparrow$) and model calibration (ECE) ($\downarrow$) of DNNs under distributional shift. An example is shown in Fig.2. 
<p align = "center">
<img src="https://raw.githubusercontent.com/hendrycks/robustness/master/assets/imagenet-c.png" width="50%">
</p>
<p align = "center">
<sup>Fig.2 An example of corrupted data
(source: <a href="https://arxiv.org/abs/1903.12261">hendrycks et al., 2019</a>)</sup>
</p>
The complete list of corruptions is attached:
<div class="grid-container">
  <div class="grid grid--px-2">
    <div class="cell cell--2"><div>brightness</div></div>
    <div class="cell cell--2"><div>gaussian blur</div></div>
    <div class="cell cell--2"><div>motion blur</div></div>
    <div class="cell cell--2"><div>spatter</div></div>
    <div class="cell cell--2"><div>contrast</div></div>
    <div class="cell cell--2"><div>gaussian noise</div></div>
    <div class="cell cell--2"><div>pixelate</div></div>
    <div class="cell cell--2"><div>speckle noise</div></div>
    <div class="cell cell--2"><div>defocus blur</div></div>
    <div class="cell cell--2"><div>glass blur</div></div>
    <div class="cell cell--2"><div>zoom blur</div></div>
    <div class="cell cell--2"><div>elastic transform</div></div>
    <div class="cell cell--2"><div>impulse noise</div></div>
    <div class="cell cell--2"><div>saturate</div></div>
    <div class="cell cell--2"><div>fog</div></div>
    <div class="cell cell--2"><div>jpeg compression</div></div>
    <div class="cell cell--2"><div>shot noise</div></div>
    <div class="cell cell--2"><div>frost</div></div>
    <div class="cell cell--2"><div>snow</div></div>
  </div>
</div>

#### Rotation Invariance ####
DNNs trained by adversarial defenses are assessed with rotated input. We are curious whether adversarial defense could generate rotation invariant predictions in this circumstance. The angles of rotation range from 0 degree to 180 degrees with the step size of 15 degrees as shown in Fig.3. The generalizability is measured by accuracy ($\uparrow$).
<p align = "center">
<img src="rotation.PNG" width="100%">
</p>
<p align = "center">
<sup>Fig.3 Rotated CIFAR-10 images</sup>
</p>
### Datasets and Models ###
Constrained by the computational resources, we select the pre-trained models on CIFAR-10. The vanilla and PGD defense model come from <a href="https://github.com/ndb796/Pytorch-Adversarial-Training-CIFAR">Pytorch-Adversarial-Training-CIFAR</a>. The DDPM defense model comes from their official <a href="https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness">repository</a>. <br/>
The vanilla and PGD defense models use ResNet-18, while the DDPM model uses ResNet-18 with pre-activation. Though the architectures are slightly different, it should not have significant impact on our conclusions.  <br/>
For out-of-distribution detection, we use Street View House Number (SVHN) and Kuzushiji-MNIST (KMNIST) as the counterpart of CIFAR-10 test set.
<p align = "center">
<img src="SVHN_KMNIST.PNG" width="80%">
</p>
<p align = "center">
<sup>Fig.4 Examples of SVHN (four images on the left) and KMNIST(four images on the right)</sup>
</p>
Naturally, CIFAR-10-C is selected as the corrupted dataset for distributional shift.
## Experiments ##
### Adversarial Robustness ###
<p align = "center">
<img src="adv_robust.PNG" width="50%">
</p>
<p align = "center">
<sup>Fig.5 Benign accuracy and robust accuracy under untargeted PGD attack of the vanilla model (<strong>vanilla</strong>), the PGD defense model (<strong>pgd_net</strong>) and the DDPM augmented model (<strong>aug_net</strong>)</sup>
</p>
From Fig.5, for the benign input, the **vanilla** model outperforms **pgd_net** by a large margin (over 10%). **pgd_net** and **aug_net** have similar performance. When we switch to adversarial input, the **vanilla** model is completely shattered and fails to classify most samples correctly. **aug_net** demonstrates the state-of-the-art adversarial resistance and achieves the best robust accuracy among the three methods, about 61%. <br/>
**pgd_net** is the first runner-up. The big gap (about 15%) of robust accuracy between the **pgd_net** and **aug_net** shows the recent advances in adversarial machine learning. <br/>
This section shows that vanilla models are vulnerable to adversarial attacks. Apart from that, it also serves to verify our models and demonstrate the effectiveness of adversarial defenses.

### Uncertainty Calibration for In-distribution Input ###
<p align = "center">
<img src="un_cali.PNG" width="50%">
</p>
<p align = "center">
<sup>Fig.6 Benign ECE and robust ECE under untargeted PGD attack of the <strong>vanilla</strong>, <strong>pgd_net</strong> and <strong>aug_net</strong></sup>
</p>
From Fig.6, in the benign setup, despite regularization from PGD training and DDPM augmentation, both of them fail to mitigate the miscalibration of DNNs. <a href="https://arxiv.org/abs/2003.03879">Chun et al., 2020</a> has a similar observation for PGD training. Unfortunately, they failed to disentangle the influence of PGD training as label smoothing is enabled in their setup. **aug_net** suffers from miscalibration more severely than **pgd_net**.<br/>
In the adversarial setup, it is not surprising that the **aug_net** has the lowest ECE error, while **vanilla** carries the highest ECE error. Intuitively, a PGD attack aims to maximize the prediction error whilst adversarial defenses could hinder such kind of sabotage.

### Out-of-distribution Detection ###
<p align = "center">
<img src="ood.PNG" width="50%">
</p>
<p align = "center">
<sup>Fig.6 Benign ECE and robust ECE under untargeted PGD attack of the <strong>vanilla</strong>, <strong>pgd_net</strong> and <strong>aug_net</strong></sup>
</p>
From Fig.7, when the out-of-distribution input comes from SVHN, **pgd_net** and **aug_net** perform worse than the **vanilla** baseline. **aug_net** is marginally worse than **pgd_net**. When outliers come from KMNIST, both adversarial defense methods fail to detect them better than **vanilla**. **aug_net**'s AUC drops significantly to approximately 0.6 and is close to random guessing (0.5). This part of the experiments suggests that adversarial defense will impart DNNs' capability to detect out-of-distribution samples.
### Robustness Under Distributional Shift ###
<p align = "center">
<img src="cifar10c_box_acc.PNG" width="49%">
<img src="cifar10c_box_ece.PNG" width="49%">
</p>
<p align = "center">
<sup>Fig.7 accuracy and ECE of <strong>vanilla</strong>, <strong>pgd_net</strong> and <strong>aug_net</strong> under different severities of corruptions</sup>
</p>
All methods are evaluated on CIFAR-10-C for five severities. Talking about the accuracy, as shown in Fig.7, all methods are affected by corruptions. when corruptions are mild (severity 0 and 1), **vanilla** is superior to **pgd_net** and **aug_net**. At higher severities, **vanilla**'s accuracy is scattering over the whole spectrum, indicating that **vanilla** is not robust under distributional shift. In contrast to **vanilla**, **pgd_net** and **aug_net** demonstrates more consistent generalization even at high severities. **aug_net** outperforms **pgd_net** marginally at all severities. <br/>  
Speaking of ECE, **pgd_net** has the lowest ECE for all severities of corruptions. At low severities (i.e. 0, 1 and 2), **vanilla** is better calibrated than **aug_net**. At high severities, it's hard to tell if **vanilla** or **aug_net** is more miscalibrated. But one thing is for sure, **vanilla** has larger variation (the distance between 0.25-quartile and 0.75-quartile) than **aug_net** as perturbation intensity increases.<br/>
<p align = "center">
<img src="cifar10c_type.PNG" width="100%">
</p>
<p align = "center">
<sup>Fig.8 Accuracy of the <strong>vanilla</strong>, <strong>pgd_net</strong> and <strong>aug_net</strong> under distributional shift. The average of five severities and the standard deviation is shown.</sup>
</p>
To investigate the sensitivity of methods to different corruptions, we group the result according to the type of corruptions. We plot the accuracy under different kinds of corruptions in Fig.8. In general, **pgd_net** and **aug_net** are similar in accuracy. **aug_net** is slightly better. In Tab.1, we summarize the situations, in which adversarial defenses are inferior ("deterioration") and superior ("improvement") to the vanilla model.<br/>
<table>
<caption>Tab.1 Sensitivity of adversarial defenses to different corruptions</caption>
  <tr>
      <th>Deterioration</th>
      <td>brightness</td>
      <td>motion blur</td>
      <td>spatter</td>
      <td nowrap="nowrap">elastic transform</td>
      <td>contrast</td>
      <td>saturate</td>
      <td>fog</td>
      <td>frost</td>
      <td>snow</td>
  </tr>
  <tr>
      <th>Improvement</th>
      <td nowrap="nowrap">Gaussian noise</td>
      <td nowrap="nowrap">speckle noise</td>
      <td nowrap="nowrap">glass blur</td>
      <td nowrap="nowrap">impulse noise</td>
      <td nowrap="nowrap">shot noise</td>
      <td  colspan=4 align=center></td>
  </tr>
</table>
As shown in Tab.1, adversarial defenses enormously improve DNNs' robustness against different kinds of noise (e.g., Gaussian and speckle) but also degenerates generalization under corruptions like weather (e.g., fog, frost)and illumination conditions (e.g., brightness, contrast). We conjecture that this can be explained by the impaired generalizability of adversarial defenses with benign input.

### Rotational Invariance ###
Human can easily recognize the type of targets without special training. We expect our DNNs to make rotation-invariant decision, though there is no explicit rotational data augmentation in training of these models. When DNNs are unlikely to give correct predictions, we expect them to know when they don't know. Do adversarial defenses help improve these?
<p align = "center">
<img src="rot_acc.PNG" width="100%">
</p>
<p align = "center">
<sup>Fig.9 Accuracy of the <strong>vanilla</strong>, <strong>pgd_net</strong> and <strong>aug_net</strong> with rotated input</sup>
</p>
<p align = "center">
<img src="rot_ece.PNG" width="100%">
</p>
<p align = "center">
<sup>Fig.10 ECE of the <strong>vanilla</strong>, <strong>pgd_net</strong> and <strong>aug_net</strong> with rotated input</sup>
</p>
As shown in Fig.9, the ranking of accuracy is the same across different angles. **vanilla** is always the champion, and **aug_net** is the first runner-up. **pgd_net** is slightly worse than **aug_net** and takes the last place. This result is consistent with the result of benign input in the section of adversarial robust. It also shows that adversarial defenses do help improve the generalization of DNNs with rotated input.
Nevertheless, adversarial defenses do improve the uncertainty calibration of DNNs. As we can see from Fig.10, **pgd_net** and **aug_net** has larger ECE than **vanilla** when there is no rotation (0 degree). With increasing angles of rotation, the output of the **vanilla** model becomes more miscalibrated. **pgd_net** also suffers from miscalibration with increasing rotation but still works slightly better than **vanilla**. Unlike **vanilla** and **pgd_net**, **aug_net** is extremely well-calibrated when rotation is larger than 30 degrees. **aug_net** is rather robust with a large rotation in input.
## Conclusion ##
In this article, we systematically evaluate the robustness of adversarial defense models in terms of adversarial robustness, uncertainty calibration for in-distribution input, out-of-distribution detection, robustness under distributional shift and rotational invariance.<br/>
In conclusion, adversarial defenses succeed in providing better generalization and uncertainty calibration with adversarial in-distribution input. Unfortunately, it is not the case with benign in-distribution input. <a href="https://arxiv.org/abs/1805.12152">Tsipras et al., 2018</a> claimed that adversarial defenses are trade-offs between robustness and standard accuracy. They also attributed the robustness to different learned feature representations. <br/>
Another drawback could be, adversarial defenses fail to distinguish between out-of-distribution samples and normal in-distribution samples. <br/>
As for distributional shift, adversarial defenses deteriorate DNNs' performance given variations of weather and illumination conditions. However, it also puts up strong resistance to different kinds of noises. Serval certifiable adversarial defense methods (e.g., <a href="https://arxiv.org/abs/1809.03113">Li et al., 2018</a> and <a href="https://arxiv.org/abs/1902.02918">Cohen et al., 2019</a>) try to linked to adversarial robustness to noise. Unfortunately, certifying is quite resource-consuming (several hours to certify a CIFAR-10 test set). The theory to explain adversarial robustness is still not fully explored and is worth digging in. <br/>
Adversarial defenses do not facilitate generalization with rotated input. However, adversarial defenses improve uncertainty calibration of DNNs while the vanilla model still suffers from miscalibration given rotated input.