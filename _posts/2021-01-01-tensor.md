---
layout: article                       # 不变
title:  "(PyTorch) Tensor(CPU, CUDA) and Numpy Array Interoperability"     # 文章标题
date:   2021-01-01 17:34:40 +0800     # 编写时间，不要超过当前系统时间，否则编译不通过
key:   tensor               # 文章的唯一 ID，不要重复了
aside:
  toc: true
category: [MLF, blog]                # 重点来了，这是类别标签（什么名字都可以，别和其他标签重了）
sidebar:
    nav: Blog
---
PyTorch tensors could be either CPU or GPU (CUDA). Numpy arrays are always in the memory (CPU). 
## Pure CPU ##
### Numpy array and CPU tensor
* When a CPU tensor is built based on a Numpy tensor, it will create a new copy in the memory. They won't have influence on each other.
    ```
    import torch
    import torch.nn as nn
    import numpy as np

    # create a numpy array and build a tensor based on it
    np_array_a = np.array([0.,1.0,2.0])
    tensor_a = torch.tensor(np_array_a)

    # modify base numpy array
    np_array_a += 1
    print(f'Numpy array A is {np_array_a}')
    print(f'Tensor A is {tensor_a.numpy()}')
    ```

    Output: 

    ```
    Numpy array A is [1. 2. 3.]
    Tensor A is [0. 1. 2.]
    ```
* When a CPU tensor creates a Numpy tensor using ```.numpy()``` method, old tensor will not be destroyed. The new Numpy array is reference to the old tensor. Modify either one will influence another.<br>
   
    ```
    tensor_b = torch.tensor([0.,1.0,2.0])
    np_b = tensor_b.numpy()
    np_b += 1
    print(f'Numpy array B is {np_b}')
    print(f'Tensor B is {tensor_b.numpy()}')

    tensor_b += 1
    print(f'Tensor B is {tensor_b.numpy()}')
    print(f'Numpy array B is {np_b}')
    ```
    Output:
    ```
    Numpy array B is [1. 2. 3.]
    Tensor B is [1. 2. 3.]
    Tensor B is [2. 3. 4.]
    Numpy array B is [2. 3. 4.]
    ```

## CUDA Tensor ##
Any CUDA tensor is by default a new copy of a CPU tensor or a Numpy array. There is no way that a Numpy could influence a CUDA tensor. However, mixing up a CUDA tensor and a Numpy array is dangerous and needs to be thoughtfully handled.<br>
A CUDA tensor is interoperable with Python scalar or another CUDA tensor. But it's not interoperable with a CPU tensor or Numpy array.

```
# initialization 
cuda_a = torch.tensor([1.,2.,3.]).cuda()
scalar_a = 2.
cuda_b = torch.tensor([2.,2.,2.]).cuda()
np_c = np.array([1,1,1])

# CUDA tensor with scalar
print(cuda_a/scalar_a)

# CUDA tensor with CUDA tensor
print(cuda_a/cuda_b)

# CUDA tensor with CPU tensor
try:
    print(cuda_a/cuda_b.cpu())
except Exception as e:
    print(f'Error: {e}')

# CUDA converted to CPU tensor with Numpy array
print(cuda_a.cpu()/np_c)

# CUDA tensor with Numpy array
try:
    print(cuda_a/np_c)
except Exception as e:
    print(f'Error: {e}')
```

output:

```
tensor([0.5000, 1.0000, 1.5000], device='cuda:0')
tensor([0.5000, 1.0000, 1.5000], device='cuda:0')
Error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
tensor([1., 2., 3.], dtype=torch.float64)
Error: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
```
